{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f44fa543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "\n",
    "# word2int = {}\n",
    "\n",
    "# dict_file = open('Data/vocabulary_semantic_modified.txt','r')\n",
    "# dict_list = dict_file.read().splitlines()\n",
    "# i=0\n",
    "# for word in dict_list:\n",
    "#     if not word in word2int:\n",
    "#         word2int[word] = i\n",
    "#         i+=1\n",
    "\n",
    "# dict_file.close()\n",
    "# new_tokens = set(word2int) - set(tokenizer.vocab.keys())\n",
    "# print(len(new_tokens))\n",
    "# print(len(tokenizer.vocab.keys()))\n",
    "# tokenizer.add_tokens(list(new_tokens))\n",
    "# print(len(tokenizer.vocab.keys()))\n",
    "# tokenizer.save_pretrained('tokenizer_Mach4/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0394e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# print(tokenizer_reloaded.vocab.get(\"<some_new_token>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebb78d28-d9e7-492d-8a26-4b60b2ea365b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<pad>', '</s>', '<unk>', 'barline', 'clef-C1', 'clef-C2', 'clef-C3', 'clef-C4', 'clef-C5']\n",
      "[0, 4, 1745, 6, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Detokenized text:  barline tie clef-C2\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab, pad_token='<pad>', start_token='<s>', end_token='</s>'):\n",
    "        self.vocab = vocab\n",
    "        self.pad_token_id = vocab[pad_token]\n",
    "        self.cls_token_id = vocab[start_token]\n",
    "        self.sep_token_id = vocab[end_token]\n",
    "        self.inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    def tokenize(self, text, max_length=None):\n",
    "        token_ids = []\n",
    "        \n",
    "        token_ids.append(self.cls_token_id)\n",
    "        \n",
    "        token_ids.extend(self.vocab[token] if token in self.vocab else self.vocab['<unk>'] for token in text.split())\n",
    "\n",
    "        token_ids.append(self.sep_token_id)\n",
    "        \n",
    "        if max_length is not None:\n",
    "            padding_length = max(0, max_length - len(token_ids))\n",
    "            token_ids.extend([self.pad_token_id] * padding_length)\n",
    "        \n",
    "        return token_ids\n",
    "\n",
    "    def detokenize(self, token_ids):\n",
    "        # Converts a list of token IDs back to a string\n",
    "        # Assuming that the start and end tokens, if present, should not be included in the detokenized output\n",
    "        assert isinstance(token_ids, list), \"token_ids must be a list\"\n",
    "        tokens = [self.inverse_vocab[token_id] for token_id in token_ids if token_id in self.inverse_vocab and token_id not in [self.cls_token_id, self.sep_token_id, self.pad_token_id]]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def batch_decode(self, batch_token_ids):\n",
    "        assert isinstance(batch_token_ids, list), \"batch_token_ids must be a list\"\n",
    "        decoded_strings = []\n",
    "        for token_ids in batch_token_ids:\n",
    "            decoded_strings.append(self.detokenize(token_ids))\n",
    "        return decoded_strings\n",
    "\n",
    "dict_file = open('vocabulary_semantic_modified.txt','r')\n",
    "dict_list = dict_file.read().splitlines()\n",
    "print(list(dict_list)[:10])\n",
    "dict_file.close()\n",
    "vocab = {tok: i for i, tok in enumerate(dict_list)}\n",
    "tokenizer = SimpleTokenizer(vocab)\n",
    "text = \"barline tie clef-C2\"\n",
    "token_ids = tokenizer.tokenize(text, max_length=256)\n",
    "print(token_ids)\n",
    "detokenized_text = tokenizer.detokenize(token_ids)\n",
    "print(\"Detokenized text: \", detokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c69f0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localscratch/bisman.17919066.0/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clef-G2 keySignature-EbM timeSignature-6/8 note-Eb5_half. barline note-Bb4_quarter. tie note-Bb4_quarter note-Ab4_eighth barline note-G4_half. barline note-Bb4_quarter. tie note-Bb4_quarter note-Ab4_eighth barline note-G4_quarter. tie note-G4_quarter note-Ab4_eighth barline note-Bb4_quarter. note-Ab4_quarter note-G4_eighth barline\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import requests\n",
    "from PIL import Image\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "def word_separator():\n",
    "    return '\\t'\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"processor_trocr-base-printed\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"Trained_Semantic\")  #Put the path of the model that you want to use for prediction\n",
    "# tokenizer = AutoTokenizer.from_pretrained('trOCR_ext_tokenizer/')\n",
    "\n",
    "sample_fullpath = 'DataMach5/Primus' + '/' + '000103290-1_1_1' + '/' + '000103290-1_1_1'\n",
    "# sample_fullpath = 'DataMach5/Primus' + '/' + '000051650-1_1_2' + '/' + '000051650-1_1_2'\n",
    "image = Image.open(sample_fullpath + '.png').convert('RGB')\n",
    "image\n",
    "\n",
    "sample_gt_fullpath = sample_fullpath + '.semantic'\n",
    "with open(sample_gt_fullpath, 'r') as sample_gt_file:\n",
    "    sample_gt_words = sample_gt_file.readline().rstrip().split(word_separator())\n",
    "    sample_gt_plain = ' '.join(sample_gt_words)\n",
    "print(sample_gt_plain) #This is the original text\n",
    "model.config.num_beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d25412bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 384, 384])\n",
      "tensor([[   0,    0,   14,  233, 1773, 1312,    4,  692, 1745,  691,  494,    4,\n",
      "         1611,    4,  692, 1115,  494,    4, 1622, 1745, 1621,  494,    4,  692,\n",
      "          501, 1603,    4,    2]], device='cuda:0')\n",
      "clef-G2 keySignature-EbM timeSignature-6/8 note-Eb5_half. barline note-Bb4_quarter. tie note-Bb4_quarter note-Ab4_eighth barline note-G4_half. barline note-Bb4_quarter. note-Db5_quarter note-Ab4_eighth barline note-G4_quarter. tie note-G4_quarter note-Ab4_eighth barline note-Bb4_quarter. note-Ab4_quarter note-G4_eighth barline\n",
      "0.07692307692307693\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "import jiwer\n",
    "def process_image(image):\n",
    "    # prepare image\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "    # print(pixel_values.shape)\n",
    "    # generate \n",
    "#     generated_ids = model.generate(pixel_values,do_sample=True,top_k=50, length_penalty = 12.0)\n",
    "    generated_ids = model.generate(pixel_values.to(device), num_beams=12, length_penalty=2, early_stopping=False)\n",
    "#     generated_ids = model.generate(pixel_values, num_beams=4, length_penalty=6, early_stopping=True)\n",
    "    # decode\n",
    "    # print(generated_ids)\n",
    "    generated_text = tokenizer.detokenize(generated_ids[0].tolist())\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# left_half, right_half = split_image_in_half(image)\n",
    "# gen_text1 = process_image(left_half)\n",
    "# print(gen_text1)\n",
    "gen_text2 = process_image(image)\n",
    "print(gen_text2) #This is the predicted Text\n",
    "print(\"Error Rate(SER): \",jiwer.wer(sample_gt_plain, gen_text2)) #This is the error rate (accuracy) of the predicted text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad4279f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 50403, 1437, 51016, 1437, 50812, 1437, 50765, 1437, 50319, 1437, 51699, 1437, 51189, 1437, 51445, 1437, 51287, 1437, 51298, 1437, 50319, 3318, 1437, 50401, 1437, 51148, 1437, 51980, 1437, 51980, 3318, 1437, 51252, 1437, 51269, 1437, 50310, 1437, 50964, 1437, 51853, 1437, 50319, 17355, 1437, 51196, 1437, 51980, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "50\n",
      "[0, 50403, 1437, 51016, 1437, 50812, 1437, 50765, 1437, 50319, 1437, 51699, 1437, 51189, 1437, 51287, 1437, 51287, 1437, 51298, 1437, 50319, 1437, 50401, 1437, 50401, 1437, 51980, 1437, 51980, 1437, 50319, 1437, 51256, 1437, 51269, 1437, 50310, 1437, 50964, 1437, 51853, 1437, 50319, 1437, 50401, 1437, 51196, 1437, 51980, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_num_labels(text):\n",
    "    labels = tokenizer(text, \n",
    "                        padding=\"max_length\", \n",
    "                        max_length=512).input_ids\n",
    "    print(labels)\n",
    "    num_labels = sum(1 for label in labels if label != tokenizer.pad_token_id and label != -100)\n",
    "    return num_labels\n",
    "\n",
    "print(get_num_labels(gen_text2))\n",
    "print(get_num_labels(sample_gt_plain))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539247a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
